{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import randint\n",
    "\n",
    "def generate_data(n_books = 3000, n_genres = 10, n_authors = 450, n_publishers = 50, n_readers = 30000, dataset_size = 100000):\n",
    "    '''\n",
    "    This function will generate a dataset with features associated to\n",
    "    book data set. The dataset will have the following columns : \n",
    "        - book_id (String) : Unique identified for the book\n",
    "        - book_rating (Integer) : A value between 0 and 10\n",
    "        - reader_id (String) : Unique identifier for the user\n",
    "        - book_genre (Integer) : An integer representing a genre for the book, \n",
    "                                 value is between 1 and 15, indicating that \n",
    "                                 there are 15 unique genres. Each book can only\n",
    "                                 have 1 genre\n",
    "        - author_id (String) : Unique identifier for the author of the book\n",
    "        - num_pages (Integer) : Random value between 70 and 500\n",
    "        - publisher_id (String) : A unique identifier for the publisher of the book\n",
    "        - publish_year (Integer) : The year of book publishing\n",
    "        - book_price (Integer) : The sale price of the book\n",
    "        - text_lang (Integer) : The language of the book - returns an integer which \n",
    "                                is mapped to some language\n",
    "        \n",
    "    params:\n",
    "        n_books (Integer) : The number of books you want the dataset to have\n",
    "        n_genres (Integer) : Number of genres to be chosen from\n",
    "        n_authors (Integer) : Number of authors to be generated\n",
    "        n_publishers (Integer) : Number of publishers for the dataset\n",
    "        n_readers (Integer) : Number of readers for the dataset\n",
    "        dataset_size (Integer) : The number of rows to be generated \n",
    "        \n",
    "    example:\n",
    "        data = generate_data()\n",
    "    '''\n",
    "    \n",
    "    d = pd.DataFrame(\n",
    "        {\n",
    "            'book_id' : [randint(1, n_books) for _ in range(dataset_size)],\n",
    "            'author_id' : [randint(1, n_authors) for _ in range(dataset_size)],\n",
    "            'book_genre' : [randint(1, n_genres) for _ in range(dataset_size)],\n",
    "            'reader_id' : [randint(1, n_readers) for _ in range(dataset_size)],\n",
    "            'num_pages' : [randint(75, 700) for _ in range(dataset_size)],\n",
    "            'book_rating' : [randint(1, 10) for _ in range(dataset_size)],\n",
    "            'publisher_id' : [randint(1, n_publishers) for _ in range(dataset_size)],\n",
    "            'publish_year' : [randint(2000, 2021) for _ in range(dataset_size)],\n",
    "            'book_price' : [randint(1, 200) for _ in range(dataset_size)],\n",
    "            'text_lang' : [randint(1,7) for _ in range(dataset_size)]\n",
    "        }\n",
    "    ).drop_duplicates()\n",
    "    return d\n",
    "  \n",
    "d = generate_data(dataset_size = 100000)\n",
    "d.to_csv('data.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import data from generate_data function (function provided above) or download the CSV from here\n",
    "##### Generate a pivot table with readers on the index and books on the column and values being the ratings\n",
    "##### Calculate similarity between items and users using svds\n",
    "##### Generate item recommendations based on user_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 10)\n",
      "   book_id       sim\n",
      "0     2393  0.167753\n",
      "1     1289  0.159118\n",
      "2      489  0.156433\n",
      "3     2536  0.155314\n",
      "4      494  0.155101\n"
     ]
    }
   ],
   "source": [
    "#Collaborative Filtering Systems\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def normalize(pred_ratings):\n",
    "    '''\n",
    "    This function will normalize the input pred_ratings\n",
    "    \n",
    "    params:\n",
    "        pred_ratings (List -> List) : The prediction ratings \n",
    "    '''\n",
    "    return (pred_ratings - pred_ratings.min()) / (pred_ratings.max() - pred_ratings.min())\n",
    "  \n",
    "def generate_prediction_df(mat, pt_df, n_factors):\n",
    "    '''\n",
    "    This function will calculate the single value decomposition of the input matrix\n",
    "    given n_factors. It will then generate and normalize the user rating predictions.\n",
    "    \n",
    "    params:\n",
    "        mat (CSR Matrix) : scipy csr matrix corresponding to the pivot table (pt_df)\n",
    "        pt_df (DataFrame) : pandas dataframe which is a pivot table\n",
    "        n_factors (Integer) : Number of singular values and vectors to compute. \n",
    "                              Must be 1 <= n_factors < min(mat.shape). \n",
    "    '''\n",
    "    \n",
    "    if not 1 <= n_factors < min(mat.shape):\n",
    "        raise ValueError(\"Must be 1 <= n_factors < min(mat.shape)\")\n",
    "        \n",
    "    # matrix factorization\n",
    "    u, s, v = svds(mat, k = n_factors)\n",
    "    s = np.diag(s)\n",
    "\n",
    "    # calculate pred ratings\n",
    "    pred_ratings = np.dot(np.dot(u, s), v) \n",
    "    pred_ratings = normalize(pred_ratings)\n",
    "    \n",
    "    # convert to df\n",
    "    pred_df = pd.DataFrame(\n",
    "        pred_ratings,\n",
    "        columns = pt_df.columns,\n",
    "        index = list(pt_df.index)\n",
    "    ).transpose()\n",
    "    return pred_df\n",
    "\n",
    "def recommend_items(pred_df, usr_id, n_recs):\n",
    "    '''\n",
    "    Given a usr_id and pred_df this function will recommend\n",
    "    items to the user.\n",
    "    \n",
    "    params:\n",
    "        pred_df (DataFrame) : generated from `generate_prediction_df` function\n",
    "        usr_id (Integer) : The user you wish to get item recommendations for\n",
    "        n_recs (Integer) : The number of recommendations you want for this user\n",
    "    '''\n",
    "    \n",
    "    usr_pred = pred_df[usr_id].sort_values(ascending = False).reset_index().rename(columns = {usr_id : 'sim'})\n",
    "    rec_df = usr_pred.sort_values(by = 'sim', ascending = False).head(n_recs)\n",
    "    return rec_df\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    # constants\n",
    "    PATH = 'data.csv'\n",
    "\n",
    "    # import data\n",
    "    df = pd.read_csv(PATH)\n",
    "    print(df.shape)\n",
    "\n",
    "    # generate a pivot table with readers on the index and books on the column and values being the ratings\n",
    "    pt_df = df.pivot_table(\n",
    "        columns = 'book_id',\n",
    "        index = 'reader_id',\n",
    "        values = 'book_rating'\n",
    "    ).fillna(0)\n",
    "\n",
    "    # convert to a csr matrix\n",
    "    mat = pt_df.values\n",
    "    mat = csr_matrix(mat)\n",
    "    \n",
    "    pred_df = generate_prediction_df(mat, pt_df, 10)\n",
    "\n",
    "    # generate recommendations\n",
    "    print(recommend_items(pred_df, 5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         author_id  reader_id  publisher_id  num_pages_norm  book_rating_norm  \\\n",
      "book_id                                                                         \n",
      "1772           275      23465            43        0.925714               1.0   \n",
      "2782           276      23511            43        0.992857               0.4   \n",
      "1213           292      24909            45        0.795714               0.7   \n",
      "2428           278      23637            45        0.762857               0.6   \n",
      "1639           257      21957            41        0.650000               0.4   \n",
      "\n",
      "         book_price_norm  2000  2001  2002  2003  ...  9  10  1  2  3  4  5  \\\n",
      "book_id                                           ...                         \n",
      "1772               0.870     0     0     0     0  ...  0   0  0  0  0  0  0   \n",
      "2782               0.700     0     0     0     0  ...  1   0  0  0  1  0  0   \n",
      "1213               0.585     0     0     0     0  ...  0   0  0  0  1  0  0   \n",
      "2428               0.305     0     0     0     0  ...  0   0  0  1  0  0  0   \n",
      "1639               0.805     0     1     0     0  ...  0   0  0  0  0  0  1   \n",
      "\n",
      "         6  7       sim  \n",
      "book_id                  \n",
      "1772     0  1  5.428124  \n",
      "2782     0  0  5.428124  \n",
      "1213     0  0  5.428124  \n",
      "2428     0  0  5.428124  \n",
      "1639     0  0  5.428124  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "#Content Based Recommendation\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm \n",
    "\n",
    "def normalize(data):\n",
    "    '''\n",
    "    This function will normalize the input data to be between 0 and 1\n",
    "    \n",
    "    params:\n",
    "        data (List) : The list of values you want to normalize\n",
    "    \n",
    "    returns:\n",
    "        The input data normalized between 0 and 1\n",
    "    '''\n",
    "    min_val = min(data)\n",
    "    if min_val < 0:\n",
    "        data = [x + abs(min_val) for x in data]\n",
    "    max_val = max(data)\n",
    "    return [x/max_val for x in data]\n",
    "\n",
    "def ohe(df, enc_col):\n",
    "    '''\n",
    "    This function will one hot encode the specified column and add it back\n",
    "    onto the input dataframe\n",
    "    \n",
    "    params:\n",
    "        df (DataFrame) : The dataframe you wish for the results to be appended to\n",
    "        enc_col (String) : The column you want to OHE\n",
    "    \n",
    "    returns:\n",
    "        The OHE columns added onto the input dataframe\n",
    "    '''\n",
    "    \n",
    "    ohe_df = pd.get_dummies(df[enc_col])\n",
    "    ohe_df.reset_index(drop = True, inplace = True)\n",
    "    return pd.concat([df, ohe_df], axis = 1)\n",
    "\n",
    "class CBRecommend():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def cosine_sim(self, v1,v2):\n",
    "        '''\n",
    "        This function will calculate the cosine similarity between two vectors\n",
    "        '''\n",
    "        return sum(dot(v1,v2)/(norm(v1)*norm(v2)))\n",
    "    \n",
    "    def recommend(self, book_id, n_rec):\n",
    "        \"\"\"\n",
    "        df (dataframe): The dataframe\n",
    "        song_id (string): Representing the song name\n",
    "        n_rec (int): amount of rec user wants\n",
    "        \"\"\"\n",
    "        \n",
    "        # calculate similarity of input book_id vector w.r.t all other vectors\n",
    "        inputVec = self.df.loc[book_id].values\n",
    "        self.df['sim']= self.df.apply(lambda x: self.cosine_sim(inputVec, x.values), axis=1)\n",
    "\n",
    "        # returns top n user specified books\n",
    "        return self.df.nlargest(columns='sim',n=n_rec)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # constants\n",
    "    PATH = 'data.csv'\n",
    "\n",
    "    # import data\n",
    "    df = pd.read_csv(PATH)\n",
    "\n",
    "    # normalize the num_pages, ratings, price columns\n",
    "    df['num_pages_norm'] = normalize(df['num_pages'].values)\n",
    "    df['book_rating_norm'] = normalize(df['book_rating'].values)\n",
    "    df['book_price_norm'] = normalize(df['book_price'].values)\n",
    "    \n",
    "    # OHE on publish_year and genre\n",
    "    df = ohe(df = df, enc_col = 'publish_year')\n",
    "    df = ohe(df = df, enc_col = 'book_genre')\n",
    "    df = ohe(df = df, enc_col = 'text_lang')\n",
    "    \n",
    "    # drop redundant columns\n",
    "    cols = ['publish_year', 'book_genre', 'num_pages', 'book_rating', 'book_price', 'text_lang']\n",
    "    df.drop(columns = cols, inplace = True)\n",
    "    df.set_index('book_id', inplace = True)\n",
    "    \n",
    "    # ran on a sample as an example\n",
    "    t = df.copy()\n",
    "    cbr = CBRecommend(df = t)\n",
    "    print(cbr.recommend(book_id = t.index[0], n_rec = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.9335\n",
      "       book_id  book_rating  num_pages  publish_year  book_price  reader_id  \\\n",
      "19363      971            9        698          2006         188       5686   \n",
      "8729      2679           10        682          2017         140      19919   \n",
      "20607     1272            9        141          2018         125       8388   \n",
      "10680     1084            4        518          2021          15      28831   \n",
      "25477     1299            4        418          2014         143      26027   \n",
      "\n",
      "       est  \n",
      "19363  5.0  \n",
      "8729   5.0  \n",
      "20607  5.0  \n",
      "10680  5.0  \n",
      "25477  5.0  \n"
     ]
    }
   ],
   "source": [
    "#Hybrid\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from surprise import SVD, Reader, Dataset, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "def hybrid(reader_id, book_id, n_recs, df, cosine_sim, svd_model):\n",
    "    '''\n",
    "    This function represents a hybrid recommendation system, it will have the following flow:\n",
    "        1. Use a content-based model (cosine_similarity) to compute the 50 most similar books\n",
    "        2. Compute the predicted ratings that the user might give these 50 books using a collaborative\n",
    "           filtering model (SVD)\n",
    "        3. Return the top n books with the highest predicted rating\n",
    "        \n",
    "    params:\n",
    "        reader_id (Integer) : The reader_id \n",
    "        book_id (Integer) : The book_id \n",
    "        n_recs (Integer) : The number of recommendations you want\n",
    "        df (DataFrame) : Original dataframe with all book information \n",
    "        cosine_sim (DataFrame) : The cosine similarity dataframe\n",
    "        svd_model (Model) : SVD model\n",
    "    '''\n",
    "    \n",
    "    # sort similarity values in decreasing order and take top 50 results\n",
    "    sim = list(enumerate(cosine_sim[int(book_id)]))\n",
    "    sim = sorted(sim, key=lambda x: x[1], reverse=True)\n",
    "    sim = sim[1:50]\n",
    "    \n",
    "    # get book metadata\n",
    "    book_idx = [i[0] for i in sim]\n",
    "    books = df.iloc[book_idx][['book_id', 'book_rating', 'num_pages', 'publish_year', 'book_price', 'reader_id']]\n",
    "    \n",
    "    # predict using the svd_model\n",
    "    books['est'] = books.apply(lambda x: svd_model.predict(reader_id, x['book_id'], x['book_rating']).est, axis = 1)\n",
    "    \n",
    "    # sort predictions in decreasing order and return top n_recs\n",
    "    books = books.sort_values('est', ascending=False)\n",
    "    return books.head(n_recs)\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    # constants\n",
    "    PATH = 'data.csv'\n",
    "\n",
    "    # import data\n",
    "    df = pd.read_csv(PATH)\n",
    "\n",
    "    # content based\n",
    "    rmat = df.pivot_table(\n",
    "        columns = 'book_id',\n",
    "        index = 'reader_id',\n",
    "        values = 'book_rating'\n",
    "    ).fillna(0)\n",
    "\n",
    "    #Compute the cosine similarity matrix \n",
    "    cosine_sim = cosine_similarity(rmat, rmat)\n",
    "    cosine_sim = pd.DataFrame(cosine_sim, index=rmat.index, columns=rmat.index)\n",
    "\n",
    "    # collaborative filtering\n",
    "    reader = Reader()\n",
    "    data = Dataset.load_from_df(df[['reader_id', 'book_id', 'book_rating']], reader)\n",
    "\n",
    "    # split data into train test\n",
    "    trainset, testset = train_test_split(data, test_size=0.3,random_state=10)\n",
    "\n",
    "    # train\n",
    "    svd = SVD()\n",
    "    svd.fit(trainset)\n",
    "\n",
    "    # run the trained model against the testset\n",
    "    test_pred = svd.test(testset)\n",
    "\n",
    "    # get RMSE\n",
    "    accuracy.rmse(test_pred, verbose=True)\n",
    "    \n",
    "    # generate recommendations\n",
    "    r_id = df['reader_id'].values[0]\n",
    "    b_id = df['book_id'].values[0]\n",
    "    n_recs = 5\n",
    "    print(hybrid(r_id, b_id, n_recs, df, cosine_sim, svd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
